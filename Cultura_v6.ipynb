{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pv52sCLBNv9s"
      },
      "outputs": [],
      "source": [
        "# Cultura_from_Sheets_v5 — Sheets → expansão → perfis → Desejado → GAP → MD + PDF\n",
        "# Requisitos: gspread, gspread_dataframe, pandas, numpy, matplotlib, tabulate\n",
        "\n",
        "# ===========================\n",
        "# Config\n",
        "# ===========================\n",
        "SPREADSHEET_INPUT_ID  = \"COLOQUE_AQUI\"   # planilha origem (aba Leads/Leads_Clean)\n",
        "SPREADSHEET_OUTPUT_ID = \"COLOQUE_AQUI\"   # planilha destino: abas CULT_*\n",
        "TAB_IN   = \"Leads_Clean\"                 # ou \"Leads\"\n",
        "\n",
        "# caminhos locais\n",
        "TAXONOMY_CSV = \"data/essencias_barrett_cvf_denison_iso_v3.csv\"\n",
        "CAT_PATHS = [\n",
        "    \"data/essencias_barrett_cvf_denison_iso_v3.csv\",   # ok usar a mesma p/ cores/chakra/camadas\n",
        "    \"data/essencias_88_enriquecido.json\",\n",
        "]\n",
        "\n",
        "# controles\n",
        "MIN_N   = 5\n",
        "SMOOTH  = 1.0\n",
        "DEFAULT_WEIGHTS = {\n",
        "    (\"preselection\",\"positive\"):    0.5,\n",
        "    (\"preselection\",\"negative\"):   -0.5,\n",
        "    (\"selection_final\",\"positive\"): 1.0,\n",
        "    (\"selection_final\",\"negative\"): -1.0,\n",
        "}\n",
        "\n",
        "# Config\n",
        "\n",
        "APPLY_MIN_N_FILTER = False  # True => filtra HEATMAP/ATTRS por times com n_min_ok\n",
        "DESIRED_FROM_FINAL_POS = True  # usa só selection_final positivos no “Desejado”\n",
        "WRITE_DESIRED_TO_SHEET = True  # grava abas CULT_DESEJADO e CULT_GAP_*\n",
        "\n",
        "# exportações\n",
        "EXPORT_MD  = True\n",
        "MD_OUT     = \"/mnt/data/report_cultura_v5.md\"\n",
        "EXPORT_PDF = True\n",
        "PDF_OUT    = \"/mnt/data/report_cultura_v5.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Auth Google (Colab)\n",
        "# ===========================\n",
        "import gspread, pandas as pd, numpy as np, os, json, re, unicodedata\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
        "from google.colab import auth as colab_auth\n",
        "colab_auth.authenticate_user()\n",
        "import google.auth\n",
        "creds, _ = google.auth.default(scopes=[\"https://www.googleapis.com/auth/spreadsheets\",\n",
        "                                       \"https://www.googleapis.com/auth/drive\"])\n",
        "gc = gspread.authorize(creds)\n",
        "assert SPREADSHEET_INPUT_ID  != \"COLOQUE_AQUI\",  \"Defina SPREADSHEET_INPUT_ID\"\n",
        "assert SPREADSHEET_OUTPUT_ID != \"COLOQUE_AQUI\", \"Defina SPREADSHEET_OUTPUT_ID\"\n",
        "ss_in  = gc.open_by_key(SPREADSHEET_INPUT_ID)\n",
        "ss_out = gc.open_by_key(SPREADSHEET_OUTPUT_ID)"
      ],
      "metadata": {
        "id": "ZLBNXaTqpzRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Utils\n",
        "# ===========================\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "def _norm(s:str)->str:\n",
        "    s = str(s or \"\").strip().lower()\n",
        "    s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
        "    s = re.sub(r\"[^a-z0-9\\s\\-\\_]\", \"\", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def parse_selection_both(sel_str):\n",
        "    parts = [p.strip() for p in str(sel_str or \"\").split(\"|\") if p.strip()]\n",
        "    pos = [p[3:].strip() for p in parts if p.startswith(\"(+)\")]\n",
        "    neg = [p[3:].strip() for p in parts if p.startswith(\"(-)\")]\n",
        "    return pos, neg\n",
        "\n",
        "def load_catalog(paths):\n",
        "    for p in paths:\n",
        "        fp = Path(p)\n",
        "        if not fp.exists(): continue\n",
        "        if fp.suffix.lower() == \".json\":\n",
        "            data = json.loads(fp.read_text(encoding=\"utf-8\"))\n",
        "            items = data if isinstance(data, list) else data.get(\"items\", [])\n",
        "            df = pd.DataFrame(items)\n",
        "        else:\n",
        "            df = pd.read_csv(fp)\n",
        "        if \"id\" not in df.columns:\n",
        "            if \"essence_id\" in df.columns: df[\"id\"] = df[\"essence_id\"]\n",
        "            elif \"name\" in df.columns:     df[\"id\"] = df[\"name\"].map(_norm)\n",
        "            else:                          df[\"id\"] = df.iloc[:,0].astype(str)\n",
        "        if \"color\" not in df.columns and \"cor\" in df.columns: df[\"color\"] = df[\"cor\"]\n",
        "        keep = [\"id\",\"color\",\"chakra\",\"camada\",\"arquetipo\",\"dominio\"]\n",
        "        for k in keep:\n",
        "            if k not in df.columns: df[k] = None\n",
        "        df = df[keep].copy()\n",
        "        df[\"id_norm\"] = df[\"id\"].astype(str).str.lower()\n",
        "        return df[[\"id_norm\",\"color\",\"chakra\",\"camada\",\"arquetipo\",\"dominio\"]]\n",
        "    return None\n",
        "\n",
        "def infer_domain_from_chakra(ch):\n",
        "    ch = str(ch or \"\").strip().lower()\n",
        "    if ch in {\"ch4\",\"4\"}: return \"Rel\"\n",
        "    if ch in {\"ch3\",\"3\",\"ch6\",\"6\"}: return \"Prof\"\n",
        "    if ch in {\"ch1\",\"1\",\"ch5\",\"5\"}: return \"Ene\"\n",
        "    return \"Prof\"\n",
        "\n",
        "def read_leads(spreadsheet, tab=\"Leads_Clean\"):\n",
        "    ws = spreadsheet.worksheet(tab)\n",
        "    df = get_as_dataframe(ws, evaluate_formulas=True, header=0).dropna(how=\"all\")\n",
        "    for c in [\"stage\",\"kind\",\"mode\",\"tenant_id\",\"team\"]:\n",
        "        if c in df.columns: df[c] = df[c].astype(str).str.lower()\n",
        "    if \"stage\" not in df.columns:\n",
        "        if \"kind\" in df.columns:\n",
        "            k = df[\"kind\"].astype(str).str.lower()\n",
        "            df[\"stage\"] = k.where(k.isin([\"preselection\",\"selection_final\"]), \"\")\n",
        "        else:\n",
        "            df[\"stage\"] = \"\"\n",
        "    for c in [\"timestamp_local\",\"timestamp\"]:\n",
        "        if c in df.columns: df[c] = pd.to_datetime(df[c], errors=\"coerce\")\n",
        "    if \"session_effective\" not in df.columns:\n",
        "        df[\"session_effective\"] = df.get(\"event_id_linked\", df.get(\"event_id\"))\n",
        "    return df\n",
        "\n",
        "def expand_all(df):\n",
        "    df2 = df[df[\"stage\"].isin([\"preselection\",\"selection_final\"])].copy()\n",
        "    out = []\n",
        "    for _, row in df2.iterrows():\n",
        "        stage = row[\"stage\"]\n",
        "        pos, neg = parse_selection_both(row.get(\"selection\"))\n",
        "        for nm, valence in [(pos,\"positive\"),(neg,\"negative\")]:\n",
        "            for name in nm:\n",
        "                out.append({\n",
        "                    \"tenant_id\": row.get(\"tenant_id\") or \"DEFAULT\",\n",
        "                    \"team\": row.get(\"team\") or \"DEFAULT\",\n",
        "                    \"session_effective\": row.get(\"session_effective\"),\n",
        "                    \"stage\": stage,\n",
        "                    \"valence\": valence,\n",
        "                    \"essence_name\": name\n",
        "                })\n",
        "    out = pd.DataFrame(out)\n",
        "    if len(out)==0: return out\n",
        "    out[\"essence_id\"] = out[\"essence_name\"].map(_norm)\n",
        "    out[\"id_norm\"] = out[\"essence_id\"].astype(str).str.lower()\n",
        "    return out\n",
        "\n",
        "def write_tab(ss, name, df):\n",
        "    try:\n",
        "        ws = ss.worksheet(name); ws.clear()\n",
        "    except Exception:\n",
        "        rows = max(100, (len(df)+10) if df is not None else 100)\n",
        "        ws = ss.add_worksheet(title=name, rows=str(rows), cols=\"50\")\n",
        "    if df is None or len(df)==0: df = pd.DataFrame([{\"info\":\"sem dados\"}])\n",
        "    set_with_dataframe(ws, df.reset_index(drop=True), include_index=False)"
      ],
      "metadata": {
        "id": "SL4yH3H8tbAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Taxonomia completa\n",
        "# ===========================\n",
        "def load_taxonomy_full(p=Path(TAXONOMY_CSV)):\n",
        "    tx = pd.read_csv(p)\n",
        "    tx.columns = [c.strip().lower() for c in tx.columns]\n",
        "    def _norm_local(s):\n",
        "        s = str(s or \"\").strip().lower()\n",
        "        s = \"\".join(c for c in unicodedata.normalize(\"NFKD\", s) if not unicodedata.combining(c))\n",
        "        s = re.sub(r\"[^a-z0-9\\s\\-\\_]\", \"\", s)\n",
        "        s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "        return s\n",
        "    tx[\"_key\"] = tx[\"essencia\"].map(_norm_local)\n",
        "    for c in [\"tema_primario\",\"barrett_principal\",\"capacidade_negocio\",\"cvf_quadrante\",\"denison_dimensao\",\n",
        "              \"cor_primaria\",\"polaridade_cor\",\"camada\",\"familia_botanica\",\"limitante\",\"barrett_adjacentes\",\n",
        "              \"peso_aspiracional\",\"chakra\",\"arquetipos\"]:\n",
        "        if c not in tx.columns: tx[c] = None\n",
        "    tx[\"peso_aspiracional\"] = pd.to_numeric(tx.get(\"peso_aspiracional\",1.0), errors=\"coerce\").fillna(1.0)\n",
        "    return tx\n",
        "\n",
        "TAX = load_taxonomy_full()"
      ],
      "metadata": {
        "id": "N3xmU_90tg6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Scoring e agregação\n",
        "# ===========================\n",
        "def score_group(rows: pd.DataFrame, weights=None, catalog: pd.DataFrame=None, smooth=1.0):\n",
        "    weights = weights or DEFAULT_WEIGHTS\n",
        "    base = rows.copy()\n",
        "\n",
        "    if catalog is not None:\n",
        "        base = base.merge(catalog, on=\"id_norm\", how=\"left\")\n",
        "\n",
        "    tx_min = TAX[[\"_key\",\"tema_primario\",\"barrett_principal\",\"capacidade_negocio\",\n",
        "                  \"cor_primaria\",\"polaridade_cor\",\"camada\",\"familia_botanica\",\n",
        "                  \"limitante\",\"chakra\"]].rename(columns={\"_key\":\"id_norm\"})\n",
        "    base = base.merge(tx_min, on=\"id_norm\", how=\"left\")\n",
        "\n",
        "    if \"dominio\" in base.columns:\n",
        "        base[\"dominio\"] = base[\"dominio\"].fillna(base[\"chakra\"].map(infer_domain_from_chakra))\n",
        "    else:\n",
        "        base[\"dominio\"] = base[\"chakra\"].map(infer_domain_from_chakra)\n",
        "\n",
        "    base[\"w\"] = base.apply(lambda r: weights.get((str(r[\"stage\"]), str(r[\"valence\"])), 0.0), axis=1)\n",
        "\n",
        "    attr_list = [\n",
        "        (\"barrett_principal\",\"Barrett\"),\n",
        "        (\"tema_primario\",\"Tema\"),\n",
        "        (\"capacidade_negocio\",\"Capacidade\"),\n",
        "        (\"cor_primaria\",\"Cor\"),\n",
        "        (\"polaridade_cor\",\"Polaridade\"),\n",
        "        (\"camada\",\"Camada\"),\n",
        "        (\"chakra\",\"Chakra\"),\n",
        "        (\"arquetipo\",\"Arquetipo\"),\n",
        "        (\"dominio\",\"Dominio\"),\n",
        "    ]\n",
        "\n",
        "    out = {}\n",
        "    for col, _label in attr_list:\n",
        "        if col not in base.columns:\n",
        "            continue\n",
        "        g = base.groupby(col, dropna=False)\n",
        "        rows_ = []\n",
        "        for k, sub in g:\n",
        "            k2 = \"—\" if pd.isna(k) or str(k).strip()==\"\" else str(k)\n",
        "            pos = (sub[\"valence\"]==\"positive\").sum()\n",
        "            neg = (sub[\"valence\"]==\"negative\").sum()\n",
        "            sc  = sub[\"w\"].sum()\n",
        "            rows_.append({\"attr\":col,\"value\":k2,\"score\":sc,\"pos\":pos,\"neg\":neg,\"count\":len(sub)})\n",
        "        out[col] = pd.DataFrame(rows_).sort_values([\"score\",\"count\"], ascending=[False,False]).reset_index(drop=True)\n",
        "\n",
        "    pre_pos = set(base[(base[\"stage\"]==\"preselection\") & (base[\"valence\"]==\"positive\")][\"id_norm\"])\n",
        "    fin_pos = set(base[(base[\"stage\"]==\"selection_final\") & (base[\"valence\"]==\"positive\")][\"id_norm\"])\n",
        "    kept = len(pre_pos & fin_pos)\n",
        "    ret = round(100*kept/max(1,len(pre_pos)),2) if len(pre_pos)>0 else None\n",
        "    neg_share = round(100*(base[\"valence\"]==\"negative\").mean(),2) if len(base) else 0.0\n",
        "\n",
        "    ent = {\"pos\": {\"pct_limitantes\":0.0,\"pct_camada2_3\":0.0},\n",
        "           \"neg\": {\"pct_limitantes\":0.0,\"pct_camada2_3\":0.0}}\n",
        "    if \"limitante\" in base.columns and \"camada\" in base.columns:\n",
        "        for v in [\"positive\",\"negative\"]:\n",
        "            sub = base[base[\"valence\"]==v]\n",
        "            if len(sub):\n",
        "                ent[v][\"pct_limitantes\"] = float((sub[\"limitante\"].astype(str)==\"1\").mean())\n",
        "                ent[v][\"pct_camada2_3\"] = float(sub[\"camada\"].astype(str).str.contains(\"Camada2|Camada3\", case=False, na=False).mean())\n",
        "\n",
        "    return out, {\"retencao_pos\":ret, \"neg_share\":neg_share, \"entropia\":ent}\n",
        "\n",
        "def aggregate_company(expanded_rows, weights=None, catalog=None, smooth=1.0, min_n=5):\n",
        "    weights = weights or DEFAULT_WEIGHTS\n",
        "    full = expanded_rows.copy()\n",
        "\n",
        "    ov = []\n",
        "    for (tenant, team), sub in full.groupby([\"tenant_id\",\"team\"], dropna=False):\n",
        "        rec = {\"tenant_id\":tenant or \"DEFAULT\", \"team\":team or \"DEFAULT\"}\n",
        "        for st,val in [(\"preselection\",\"positive\"),(\"preselection\",\"negative\"),\n",
        "                       (\"selection_final\",\"positive\"),(\"selection_final\",\"negative\")]:\n",
        "            rec[f\"{st}_{val}\"] = ((sub[\"stage\"]==st) & (sub[\"valence\"]==val)).sum()\n",
        "        rec[\"sessions\"] = sub[\"session_effective\"].nunique() if \"session_effective\" in sub.columns else None\n",
        "        ov.append(rec)\n",
        "    OVERVIEW = pd.DataFrame(ov).sort_values([\"tenant_id\",\"team\"]).reset_index(drop=True)\n",
        "\n",
        "    blocks, reten, vies, attrs_rows = [], [], [], []\n",
        "    for (tenant, team), sub in full.groupby([\"tenant_id\",\"team\"], dropna=False):\n",
        "        subN = len(sub)\n",
        "        sc_tables, metrics = score_group(sub, weights=weights, catalog=catalog, smooth=smooth)\n",
        "        for attr, df_attr in sc_tables.items():\n",
        "            df2 = df_attr.copy()\n",
        "            df2.insert(0,\"team\", team or \"DEFAULT\")\n",
        "            df2.insert(0,\"tenant_id\", tenant or \"DEFAULT\")\n",
        "            blocks.append(df2)\n",
        "            for _, r in df2.iterrows():\n",
        "                attrs_rows.append({\"tenant_id\":tenant or \"DEFAULT\",\"team\":team or \"DEFAULT\",\n",
        "                                   \"attr\":r[\"attr\"],\"value\":r[\"value\"],\n",
        "                                   \"score\":r[\"score\"],\"count\":r[\"count\"],\"pos\":r[\"pos\"],\"neg\":r[\"neg\"]})\n",
        "        reten.append({\"tenant_id\":tenant or \"DEFAULT\",\"team\":team or \"DEFAULT\",\"retencao_pos\":metrics[\"retencao_pos\"]})\n",
        "        neg_pct = round(100*(sub[\"valence\"]==\"negative\").mean(),2) if subN else 0.0\n",
        "        vies.append({\"tenant_id\":tenant or \"DEFAULT\",\"team\":team or \"DEFAULT\",\n",
        "                     \"n_registros\":subN,\n",
        "                     \"n_sessoes\": sub[\"session_effective\"].nunique() if \"session_effective\" in sub.columns else None,\n",
        "                     \"pct_negativos\":neg_pct,\n",
        "                     \"n_min_ok\": subN >= min_n})\n",
        "\n",
        "    HEATMAP = pd.concat(blocks, ignore_index=True) if blocks else pd.DataFrame(columns=[\"tenant_id\",\"team\",\"attr\",\"value\",\"score\",\"pos\",\"neg\",\"count\"])\n",
        "    RETENCAO = pd.DataFrame(reten).sort_values([\"tenant_id\",\"team\"]).reset_index(drop=True)\n",
        "    VIESES   = pd.DataFrame(vies).sort_values([\"tenant_id\",\"team\"]).reset_index(drop=True)\n",
        "    ATTRS    = pd.DataFrame(attrs_rows).sort_values([\"tenant_id\",\"team\",\"attr\",\"score\"], ascending=[True,True,True,False]).reset_index(drop=True)\n",
        "    return {\"OVERVIEW\": OVERVIEW, \"HEATMAP\": HEATMAP, \"RETENCAO\": RETENCAO, \"VIESES\": VIESES, \"ATTRS\": ATTRS}"
      ],
      "metadata": {
        "id": "RoqHFWOGtk7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Execução principal\n",
        "# ===========================\n",
        "df = read_leads(ss_in, TAB_IN)\n",
        "df = df.dropna(subset=[\"selection\"])\n",
        "df = df[df[\"stage\"].isin([\"preselection\",\"selection_final\"])]\n",
        "\n",
        "expanded = []\n",
        "for _, row in df.iterrows():\n",
        "    stage = row[\"stage\"]\n",
        "    pos, neg = parse_selection_both(row.get(\"selection\"))\n",
        "    for nm, valence in [(pos,\"positive\"),(neg,\"negative\")]:\n",
        "        for name in nm:\n",
        "            expanded.append({\n",
        "                \"tenant_id\": row.get(\"tenant_id\") or \"DEFAULT\",\n",
        "                \"team\": row.get(\"team\") or \"DEFAULT\",\n",
        "                \"session_effective\": row.get(\"session_effective\"),\n",
        "                \"stage\": stage,\n",
        "                \"valence\": valence,\n",
        "                \"essence_name\": name\n",
        "            })\n",
        "expanded = pd.DataFrame(expanded)\n",
        "if len(expanded):\n",
        "    expanded[\"essence_id\"] = expanded[\"essence_name\"].map(_norm)\n",
        "    expanded[\"id_norm\"] = expanded[\"essence_id\"].astype(str).str.lower()\n",
        "print(\"Registros expandidos:\", len(expanded))\n",
        "\n",
        "CAT = load_catalog(CAT_PATHS)\n",
        "packs = aggregate_company(expanded, catalog=CAT, min_n=MIN_N, smooth=SMOOTH)\n",
        "\n",
        "write_tab(ss_out, \"CULT_OVERVIEW\", packs[\"OVERVIEW\"])\n",
        "write_tab(ss_out, \"CULT_HEATMAP\",  packs[\"HEATMAP\"])\n",
        "write_tab(ss_out, \"CULT_RETENCAO\", packs[\"RETENCAO\"])\n",
        "write_tab(ss_out, \"CULT_VIESES\",   packs[\"VIESES\"])\n",
        "write_tab(ss_out, \"CULT_ATTRS\",    packs[\"ATTRS\"])"
      ],
      "metadata": {
        "id": "NnBFrk8QtqKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Desejado + GAP\n",
        "# ===========================\n",
        "CAMADA1_SET = {\n",
        "    \"blue china orchid\",\"cowslip orchid\",\"donkey orchid\",\"rabbit orchid\",\"shy blue orchid\",\n",
        "    \"pink fairy orchid\",\"white nymph water lily\",\"purple nymph water lily\",\n",
        "    \"fringed mantis orchid\",\"hybrid pink fairy cowslip orchid\",\"starts spider orchid\",\"white spider orchid\"\n",
        "}\n",
        "CAMADA3_SET = {\n",
        "    \"candle of life\",\"purple enamel orchid\",\"giving hands\",\"leafless orchid\",\n",
        "    \"pink fountain triggerplant\",\"pink trumpet flower\",\"red beak orchid\",\n",
        "    \"reed triggerplant\",\"wa smokebush\",\"silver princess\"\n",
        "}\n",
        "TEMAS_ALVO = {\"Foco/Clareza\",\"Conversas/Feedback\",\"Reconhecimento\",\"Energia\"}\n",
        "\n",
        "def _top_n_keys(expanded_rows, n=10):\n",
        "    return expanded_rows[\"id_norm\"].value_counts().head(n).index.tolist()\n",
        "\n",
        "def build_desired_auto(expanded_rows, tax_df, k_total=12):\n",
        "    freq = expanded_rows[\"id_norm\"].value_counts()\n",
        "    pool = tax_df.set_index(\"_key\").loc[freq.index].copy()\n",
        "    pool[\"freq\"] = freq.values\n",
        "    pool = pool[~pool[\"camada\"].astype(str).str.contains(\"Camada3\", case=False, na=False)].copy()\n",
        "    def prefer_equivalents(lst):\n",
        "        names = set(lst)\n",
        "        if \"pincushion hakea\" in names:\n",
        "            names.discard(\"pincushion hakea\")\n",
        "            for alt in [\"correa\",\"ribbon pea\"]:\n",
        "                if alt in tax_df[\"_key\"].values: names.add(alt)\n",
        "        return list(names)\n",
        "    top10 = _top_n_keys(expanded_rows, n=10)\n",
        "    top10_c1 = [k for k in top10 if k in CAMADA1_SET]\n",
        "    desejado = top10_c1[:]\n",
        "    if len(top10_c1)==0:\n",
        "        c1_all = pool[pool.index.isin(CAMADA1_SET)].sort_values(\"freq\", ascending=False)\n",
        "        if len(c1_all)>0: desejado.append(c1_all.index[0])\n",
        "    themed = pool[pool[\"tema_primario\"].isin(TEMAS_ALVO)].sort_values([\"tema_primario\",\"freq\"], ascending=[True,False])\n",
        "    for k in themed.index:\n",
        "        if len(desejado) >= k_total: break\n",
        "        if k in desejado: continue\n",
        "        if \"Camada2\" in str(pool.loc[k,\"camada\"]):\n",
        "            alt_c1 = pool.index.intersection(CAMADA1_SET)\n",
        "            same = pool.loc[alt_c1][pool.loc[alt_c1,\"tema_primario\"]==pool.loc[k,\"tema_primario\"]]\n",
        "            if len(same)>0: continue\n",
        "        desejado.append(k)\n",
        "    if len(desejado) < k_total:\n",
        "        resto = [k for k in pool.sort_values(\"freq\", ascending=False).index if k not in desejado]\n",
        "        desejado += resto[:(k_total - len(desejado))]\n",
        "    desejado = prefer_equivalents(desejado)\n",
        "    des = tax_df.set_index(\"_key\").loc[desejado].reset_index()\n",
        "    return des\n",
        "\n",
        "def dist_from_keys(keys, col, tax_df):\n",
        "    meta = tax_df.set_index(\"_key\").loc[keys]\n",
        "    vc = meta[col].value_counts()\n",
        "    return (vc / vc.sum()).sort_values(ascending=False) if vc.sum()>0 else vc\n",
        "\n",
        "def gap_table(cur_dist, des_dist, title):\n",
        "    idx = sorted(set(cur_dist.index) | set(des_dist.index))\n",
        "    df = pd.DataFrame({\n",
        "        \"Atual %\":    [float(cur_dist.get(i,0))*100 for i in idx],\n",
        "        \"Desejado %\": [float(des_dist.get(i,0))*100 for i in idx],\n",
        "    }, index=idx)\n",
        "    df[\"Gap (pp)\"] = (df[\"Desejado %\"] - df[\"Atual %\"]).round(1)\n",
        "    df.index.name = title\n",
        "    return df.round(1)\n",
        "\n",
        "expanded_enriched = expanded.merge(\n",
        "    TAX[[\"_key\",\"essencia\",\"tema_primario\",\"barrett_principal\",\"capacidade_negocio\",\n",
        "         \"cor_primaria\",\"polaridade_cor\",\"camada\",\"familia_botanica\",\"limitante\",\"chakra\"]]\n",
        "    .rename(columns={\"_key\":\"id_norm\"}), on=\"id_norm\", how=\"left\"\n",
        ")\n",
        "\n",
        "cur_bar = expanded_enriched[\"barrett_principal\"].value_counts()\n",
        "cur_bar = (cur_bar/cur_bar.sum()).sort_values(ascending=False) if cur_bar.sum()>0 else cur_bar\n",
        "cur_tema = expanded_enriched[\"tema_primario\"].value_counts()\n",
        "cur_tema = (cur_tema/cur_tema.sum()).sort_values(ascending=False) if cur_tema.sum()>0 else cur_tema\n",
        "cur_cap = expanded_enriched[\"capacidade_negocio\"].value_counts()\n",
        "cur_cap = (cur_cap/cur_cap.sum()).sort_values(ascending=False) if cur_cap.sum()>0 else cur_cap\n",
        "\n",
        "DES = build_desired_auto(expanded_enriched, TAX, k_total=12)\n",
        "DES.to_csv(\"/mnt/data/desejado_auto.csv\", index=False)\n",
        "\n",
        "des_bar = dist_from_keys(DES[\"_key\"], \"barrett_principal\", TAX)\n",
        "des_tema = dist_from_keys(DES[\"_key\"], \"tema_primario\", TAX)\n",
        "des_cap  = dist_from_keys(DES[\"_key\"], \"capacidade_negocio\", TAX)\n",
        "\n",
        "GAP_BARRETT = gap_table(cur_bar, des_bar, \"Nível Barrett\")\n",
        "GAP_TEMAS   = gap_table(cur_tema, des_tema, \"Tema\")\n",
        "GAP_CAPS    = gap_table(cur_cap, des_cap, \"Capacidade\")\n",
        "\n",
        "GAP_BARRETT.to_csv(\"/mnt/data/gap_barrett.csv\")\n",
        "GAP_TEMAS.to_csv(\"/mnt/data/gap_temas.csv\")\n",
        "GAP_CAPS.to_csv(\"/mnt/data/gap_capacidades.csv\")"
      ],
      "metadata": {
        "id": "mb25RL9Httnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Export MD (executivo)\n",
        "# ===========================\n",
        "def md_tbl(df: pd.DataFrame):\n",
        "    if df is None or len(df)==0: return \"_sem dados_\\n\"\n",
        "    return df.to_markdown(index=False) + \"\\n\"\n",
        "\n",
        "if EXPORT_MD:\n",
        "    lines = []\n",
        "    lines.append(\"# Cultura — Relatório Executivo v5\\n\\n\")\n",
        "    lines.append(\"## Vieses de Coleta\\n\")\n",
        "    lines.append(md_tbl(packs[\"VIESES\"]))\n",
        "    lines.append(\"## Retenção pré → final (+)\\n\")\n",
        "    lines.append(md_tbl(packs[\"RETENCAO\"]))\n",
        "    lines.append(\"## Maiores saldos por grupo\\n\")\n",
        "    dfh = packs[\"HEATMAP\"].copy()\n",
        "    if len(dfh):\n",
        "        dfh[\"saldo\"] = dfh[\"pos\"] - dfh[\"neg\"]\n",
        "        top = dfh.sort_values([\"tenant_id\",\"team\",\"saldo\",\"score\",\"count\"],\n",
        "                              ascending=[True,True,False,False,False]) \\\n",
        "                 .groupby([\"tenant_id\",\"team\"]).head(8)\n",
        "        lines.append(md_tbl(top[[\"tenant_id\",\"team\",\"attr\",\"value\",\"saldo\",\"score\",\"count\",\"pos\",\"neg\"]]))\n",
        "    else:\n",
        "        lines.append(\"_sem dados_\\n\")\n",
        "    lines.append(\"## Destaques globais por atributo (score)\\n\")\n",
        "    attrs = packs[\"ATTRS\"]\n",
        "    if len(attrs):\n",
        "        tops = []\n",
        "        for attr in [\"cor_primaria\",\"chakra\",\"camada\",\"arquetipo\",\"dominio\",\"tema_primario\",\"barrett_principal\",\"capacidade_negocio\"]:\n",
        "            col = \"attr\" if \" \" not in attr else attr\n",
        "            sub = attrs[attrs[\"attr\"].isin([attr, attr.replace(\" \",\"_\")])].sort_values([\"score\",\"count\"], ascending=[False,False]).head(5)\n",
        "            if len(sub): tops.append(sub.assign(attr_group=attr)[[\"attr_group\",\"tenant_id\",\"team\",\"value\",\"score\",\"count\"]])\n",
        "        if tops: lines.append(md_tbl(pd.concat(tops, ignore_index=True)))\n",
        "        else:    lines.append(\"_sem dados_\\n\")\n",
        "    else:\n",
        "        lines.append(\"_sem dados_\\n\")\n",
        "\n",
        "    # Desejado + GAP\n",
        "    lines.append(\"## Desejado automático (12)\\n\")\n",
        "    lines.append(DES[[\"essencia\",\"tema_primario\",\"barrett_principal\",\"camada\"]].to_markdown(index=False) + \"\\n\")\n",
        "    lines.append(\"## GAP — Barrett\\n\")\n",
        "    lines.append(GAP_BARRETT.to_markdown() + \"\\n\")\n",
        "    lines.append(\"## GAP — Temas\\n\")\n",
        "    lines.append(GAP_TEMAS.to_markdown() + \"\\n\")\n",
        "    lines.append(\"## GAP — Capacidades\\n\")\n",
        "    lines.append(GAP_CAPS.to_markdown() + \"\\n\")\n",
        "\n",
        "    with open(MD_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\".join(lines))\n",
        "    print(\"MD salvo em:\", MD_OUT)"
      ],
      "metadata": {
        "id": "1jvG6gxrtx8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Export PDF (executivo)\n",
        "# ===========================\n",
        "if EXPORT_PDF:\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "    def _bar(ax, series, title):\n",
        "        ax.clear()\n",
        "        if series is None or len(series)==0:\n",
        "            ax.axis('off'); ax.set_title(title+\" (sem dados)\", pad=6); return\n",
        "        ax.bar(list(map(str, series.index)), series.values)\n",
        "        ax.set_title(title, pad=6)\n",
        "        ax.tick_params(axis='x', rotation=30, labelsize=8)\n",
        "        ax.margins(x=0.02)\n",
        "\n",
        "    with PdfPages(PDF_OUT) as pdf:\n",
        "        # Página 1 — Sumário\n",
        "        fig = plt.figure(figsize=(8.27,11.69))\n",
        "        fig.suptitle(\"Cultura — Relatório Executivo v5\", y=0.98)\n",
        "        ax1 = fig.add_axes([0.08,0.76,0.84,0.18])\n",
        "        ax2 = fig.add_axes([0.08,0.54,0.84,0.18])\n",
        "        ax3 = fig.add_axes([0.08,0.32,0.84,0.18])\n",
        "        ax4 = fig.add_axes([0.08,0.10,0.84,0.18])\n",
        "\n",
        "        _bar(ax1, cur_bar, \"Barrett — atual\")\n",
        "        _bar(ax2, cur_tema, \"Temas — atual\")\n",
        "        _bar(ax3, cur_cap,  \"Capacidades — atual\")\n",
        "\n",
        "        ent = packs[\"VIESES\"].copy()\n",
        "        txt = []\n",
        "        if \"RETENCAO\" in packs and len(packs[\"RETENCAO\"]):\n",
        "            r = packs[\"RETENCAO\"]\n",
        "            txt.append(f\"Retenção (+) mediana: {r['retencao_pos'].median():.1f}%\" if r['retencao_pos'].notna().any() else \"Retenção (+): s/dados\")\n",
        "        if len(packs[\"VIESES\"]):\n",
        "            v = packs[\"VIESES\"]\n",
        "            txt.append(f\"Negativos médio: {v['pct_negativos'].mean():.1f}%\")\n",
        "            txt.append(f\"Times com N≥{MIN_N}: {(v['n_min_ok']).mean()*100:.0f}%\")\n",
        "        ax4.axis('off')\n",
        "        ax4.text(0.0, 0.9, \"Resumo\", fontsize=12, weight='bold')\n",
        "        y = 0.85\n",
        "        for line in txt:\n",
        "            ax4.text(0.0, y, f\"• {line}\", fontsize=10); y -= 0.08\n",
        "\n",
        "        pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
        "\n",
        "        # Página 2 — Desejado e GAPs\n",
        "        fig = plt.figure(figsize=(8.27,11.69))\n",
        "        ax1 = fig.add_axes([0.08,0.76,0.84,0.18])\n",
        "        ax2 = fig.add_axes([0.08,0.54,0.84,0.18])\n",
        "        ax3 = fig.add_axes([0.08,0.32,0.84,0.18])\n",
        "        ax4 = fig.add_axes([0.08,0.10,0.84,0.18])\n",
        "\n",
        "        _bar(ax1, des_bar, \"Barrett — desejado\")\n",
        "        _bar(ax2, des_tema, \"Temas — desejado\")\n",
        "        _bar(ax3, des_cap,  \"Capacidades — desejado\")\n",
        "\n",
        "        # mini tabela textual com top 6 do desejado\n",
        "        ax4.axis('off')\n",
        "        ax4.text(0.0, 0.95, \"Desejado automático (Top 6)\", fontsize=12, weight='bold')\n",
        "        for i,(ess,tema,bar,cam) in enumerate(DES[[\"essencia\",\"tema_primario\",\"barrett_principal\",\"camada\"]].head(6).itertuples(index=False), start=1):\n",
        "            ax4.text(0.0, 0.95 - 0.12*i, f\"{i}. {ess} — {tema} | {bar} | {cam}\", fontsize=9)\n",
        "\n",
        "        pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
        "\n",
        "        # Página 3 — GAPs em barras\n",
        "        def _gap_to_series(df, top=8):\n",
        "            s = df[\"Gap (pp)\"].sort_values(ascending=False)\n",
        "            return s.head(top)\n",
        "\n",
        "        fig = plt.figure(figsize=(8.27,11.69))\n",
        "        ax1 = fig.add_axes([0.08,0.68,0.84,0.26])\n",
        "        ax2 = fig.add_axes([0.08,0.37,0.84,0.26])\n",
        "        ax3 = fig.add_axes([0.08,0.06,0.84,0.26])\n",
        "\n",
        "        _bar(ax1, _gap_to_series(GAP_BARRETT), \"GAP — Barrett (pp) maiores\")\n",
        "        _bar(ax2, _gap_to_series(GAP_TEMAS),   \"GAP — Temas (pp) maiores\")\n",
        "        _bar(ax3, _gap_to_series(GAP_CAPS),    \"GAP — Capacidades (pp) maiores\")\n",
        "\n",
        "        pdf.savefig(fig, bbox_inches='tight'); plt.close(fig)\n",
        "\n",
        "    print(\"PDF salvo em:\", PDF_OUT)\n",
        "\n",
        "print(\"Concluído.\")"
      ],
      "metadata": {
        "id": "hKZ01133t2MI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Patches mínimos para v5.1:\n",
        "\n",
        "Patch 2 — TAX: mapear “arquetipos” → “arquetipo”\n",
        "\n",
        "Após TAX = load_taxonomy_full():\n",
        "\n",
        "if \"arquetipo\" not in TAX.columns and \"arquetipos\" in TAX.columns:\n",
        "    TAX[\"arquetipo\"] = TAX[\"arquetipos\"]\n",
        "\n",
        "Patch 3 — score_group: proporções suavizadas\n",
        "\n",
        "Substitua o loop que monta rows_ por esta versão:\n",
        "\n",
        "for k, sub in g:\n",
        "    k2  = \"—\" if pd.isna(k) or str(k).strip()==\"\" else str(k)\n",
        "    pos = (sub[\"valence\"]==\"positive\").sum()\n",
        "    neg = (sub[\"valence\"]==\"negative\").sum()\n",
        "    sc  = sub[\"w\"].sum()\n",
        "    denom = pos + neg + 2*SMOOTH\n",
        "    rows_.append({\n",
        "        \"attr\": col, \"value\": k2, \"score\": sc, \"pos\": pos, \"neg\": neg, \"count\": len(sub),\n",
        "        \"pos_rate\": round((pos + SMOOTH)/denom, 4),\n",
        "        \"neg_rate\": round((neg + SMOOTH)/denom, 4),\n",
        "    })\n",
        "\n",
        "Patch 4 — ENTROPIA real e saída\n",
        "\n",
        "Dentro de score_group após neg_share, adicione:\n",
        "\n",
        "from math import log2\n",
        "def _ent(v):\n",
        "    if not len(v): return None\n",
        "    total = sum(v) + SMOOTH*len(v)\n",
        "    H = 0.0\n",
        "    for c in v:\n",
        "        p = (c+SMOOTH)/total\n",
        "        H += -p*(log2(p) if p>0 else 0.0)\n",
        "    Hmax = log2(len(v)) if len(v)>1 else 1.0\n",
        "    return round(H/Hmax, 4)\n",
        "\n",
        "finp = base[(base[\"stage\"]==\"selection_final\") & (base[\"valence\"]==\"positive\")]\n",
        "ent_dom   = _ent(finp[\"dominio\"].value_counts().tolist())\n",
        "ent_chak  = _ent(finp[\"chakra\"].value_counts().tolist())\n",
        "\n",
        "\n",
        "E no return inclua:\n",
        "\n",
        "return out, {\"retencao_pos\": ret, \"neg_share\": neg_share,\n",
        "             \"entropy_dom_final\": ent_dom, \"entropy_chakra_final\": ent_chak}\n",
        "\n",
        "Patch 5 — aggregate_company: aplicar MIN_N e gerar CULT_ENTROPIA\n",
        "\n",
        "Troque a assinatura para:\n",
        "\n",
        "def aggregate_company(expanded_rows, weights=None, catalog=None, smooth=1.0, min_n=5, apply_min_filter=False):\n",
        "\n",
        "\n",
        "Após montar VIESES, crie ENTROPIA:\n",
        "\n",
        "ent_rows.append({\"tenant_id\": t, \"team\": m,\n",
        "                 \"entropy_dom_final\": metrics[\"entropy_dom_final\"],\n",
        "                 \"entropy_chakra_final\": metrics[\"entropy_chakra_final\"]})\n",
        "\n",
        "\n",
        "Depois de construir HEATMAP/ATTRS, aplique filtro se apply_min_filter:\n",
        "\n",
        "if apply_min_filter:\n",
        "    ok = set(map(tuple, VIESES.query(\"n_min_ok == True\")[[\"tenant_id\",\"team\"]].to_numpy()))\n",
        "    if len(HEATMAP): HEATMAP = HEATMAP[[ (r.tenant_id, r.team) in ok for _,r in HEATMAP.iterrows() ]]\n",
        "    if len(ATTRS):   ATTRS   = ATTRS[[   (r.tenant_id, r.team) in ok for _,r in ATTRS.iterrows() ]]\n",
        "ENTROPIA = pd.DataFrame(ent_rows).sort_values([\"tenant_id\",\"team\"]).reset_index(drop=True)\n",
        "return {\"OVERVIEW\": OVERVIEW, \"HEATMAP\": HEATMAP, \"RETENCAO\": RETENCAO, \"VIESES\": VIESES, \"ATTRS\": ATTRS, \"ENTROPIA\": ENTROPIA}\n",
        "\n",
        "\n",
        "No call:\n",
        "\n",
        "packs = aggregate_company(expanded, catalog=CAT, min_n=MIN_N, smooth=SMOOTH, apply_min_filter=APPLY_MIN_N_FILTER)\n",
        "\n",
        "\n",
        "E escreva a nova aba:\n",
        "\n",
        "write_tab(ss_out, \"CULT_ENTROPIA\", packs[\"ENTROPIA\"])\n",
        "\n",
        "Patch 6 — “Desejado” focado em FINAL(+)\n",
        "\n",
        "No bloco Desejado + GAP, troque:\n",
        "\n",
        "base_for_desired = expanded_enriched\n",
        "if DESIRED_FROM_FINAL_POS:\n",
        "    base_for_desired = base_for_desired[(base_for_desired[\"stage\"]==\"selection_final\") & (base_for_desired[\"valence\"]==\"positive\")]\n",
        "DES = build_desired_auto(base_for_desired, TAX, k_total=12)\n",
        "\n",
        "Patch 7 — Remover sets frágeis\n",
        "\n",
        "Dentro de build_desired_auto, elimine dependência de CAMADA1_SET/CAMADA3_SET.\n",
        "Use TAX:\n",
        "\n",
        "pool = tax_df.set_index(\"_key\").loc[freq.index].copy()\n",
        "pool[\"freq\"] = freq.values\n",
        "# priorizar Camada1 e evitar Camada3:\n",
        "pool_c1 = pool[pool[\"camada\"].astype(str).str.contains(\"Camada1\", case=False, na=False)]\n",
        "pool_noc3 = pool[~pool[\"camada\"].astype(str).str.contains(\"Camada3\", case=False, na=False)]\n",
        "top_c1 = pool_c1.sort_values(\"freq\", ascending=False).index.tolist()\n",
        "desejado = top_c1[:4]  # sementes\n",
        "for k in pool_noc3.sort_values(\"freq\", ascending=False).index:\n",
        "    if k in desejado: continue\n",
        "    desejado.append(k)\n",
        "    if len(desejado) >= k_total: break\n",
        "des = tax_df.set_index(\"_key\").loc[desejado].reset_index()\n",
        "return des\n",
        "\n",
        "Patch 8 — Escrever Desejado e GAP na planilha\n",
        "\n",
        "Após gerar DES, GAP_*:\n",
        "\n",
        "if WRITE_DESIRED_TO_SHEET:\n",
        "    write_tab(ss_out, \"CULT_DESEJADO\", DES[[\"essencia\",\"tema_primario\",\"barrett_principal\",\"camada\"]])\n",
        "    write_tab(ss_out, \"CULT_GAP_BARRETT\", GAP_BARRETT.reset_index())\n",
        "    write_tab(ss_out, \"CULT_GAP_TEMAS\",   GAP_TEMAS.reset_index())\n",
        "    write_tab(ss_out, \"CULT_GAP_CAPS\",    GAP_CAPS.reset_index())\n",
        "\n",
        "Patch 9 — MD: entropia e taxas\n",
        "\n",
        "No MD, após “Destaques globais…”:\n",
        "\n",
        "lines.append(\"## Entropia por time (FINAL +)\\n\")\n",
        "lines.append(md_tbl(packs[\"ENTROPIA\"]))\n",
        "lines.append(\"## HEATMAP com proporções suavizadas\\n\")\n",
        "cols = [\"tenant_id\",\"team\",\"attr\",\"value\",\"score\",\"count\",\"pos\",\"neg\",\"pos_rate\",\"neg_rate\"]\n",
        "hm = packs[\"HEATMAP\"][cols] if set(cols).issubset(packs[\"HEATMAP\"].columns) else packs[\"HEATMAP\"]\n",
        "lines.append(md_tbl(hm))\n",
        "\n",
        "\n",
        "Resultado: v5.1 mantém tudo que você já tem e adiciona N-mínimo efetivo, proporções estáveis, entropia e rastreabilidade do “Desejado+GAP” na própria Sheets."
      ],
      "metadata": {
        "id": "jaTgNf7FOjE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Aplique no seu notebook v5 (ou v5.1). Use exatamente os blocos abaixo.\n",
        "\n",
        "PATCH A — Config extra\n",
        "\n",
        "Colar logo após sua célula de Config.\n",
        "\n",
        "# --- filtros e escrita extra ---\n",
        "APPLY_MIN_N_FILTER = False          # True => filtra HEATMAP/ATTRS para times com n_min_ok\n",
        "DESIRED_FROM_FINAL_POS = True       # usa só selection_final (+) para construir \"Desejado\"\n",
        "WRITE_DESIRED_TO_SHEET = True       # grava abas CULT_DESEJADO e CULT_GAP_*\n",
        "MODE_FILTER_ENABLED = False         # True => aplica filtro por mode\n",
        "MODE_ALLOWED = {\"online\", \"terapeuta\"}  # ajuste conforme necessário\n",
        "\n",
        "PATCH B — Validação de taxonomia\n",
        "\n",
        "Inserir uma nova célula após carregar TAX = load_taxonomy_full().\n",
        "\n",
        "# Mapear arquetipos → arquetipo se necessário\n",
        "if \"arquetipo\" not in TAX.columns and \"arquetipos\" in TAX.columns:\n",
        "    TAX[\"arquetipo\"] = TAX[\"arquetipos\"]\n",
        "\n",
        "# Validação de cobertura do dicionário\n",
        "def validate_taxonomy(expanded_df, tax_df):\n",
        "    keys = expanded_df[\"id_norm\"].dropna().unique()\n",
        "    tx_keys = set(tax_df[\"_key\"].astype(str).str.lower()) if \"_key\" in tax_df.columns else set()\n",
        "    missing = [k for k in keys if k not in tx_keys]\n",
        "    # checar campos críticos\n",
        "    need_cols = [\"tema_primario\",\"barrett_principal\",\"capacidade_negocio\",\"cor_primaria\",\"polaridade_cor\",\"camada\",\"chakra\",\"arquetipo\"]\n",
        "    coverage = []\n",
        "    # juntar meta\n",
        "    meta = expanded_df.merge(\n",
        "        tax_df.rename(columns={\"_key\":\"id_norm\"})[\n",
        "            [\"id_norm\",\"essencia\"] + [c for c in need_cols if c in tax_df.columns]\n",
        "        ],\n",
        "        on=\"id_norm\", how=\"left\"\n",
        "    )\n",
        "    grp = meta.groupby(\"id_norm\", dropna=False)\n",
        "    for k, sub in grp:\n",
        "        row = {\"id_norm\": k, \"essencia\": sub[\"essencia\"].dropna().iloc[0] if \"essencia\" in sub else None}\n",
        "        for c in need_cols:\n",
        "            row[f\"has_{c}\"] = bool(sub[c].notna().any()) if c in sub.columns else False\n",
        "        coverage.append(row)\n",
        "    cov_df = pd.DataFrame(coverage)\n",
        "    return pd.DataFrame({\"missing_keys\": missing}), cov_df\n",
        "\n",
        "MISSING, TAX_COVER = validate_taxonomy(expanded, TAX)\n",
        "# escreve abas de validação (opcional)\n",
        "try:\n",
        "    write_tab(ss_out, \"CULT_TAX_MISSING\", MISSING)\n",
        "    write_tab(ss_out, \"CULT_TAX_COVER\", TAX_COVER)\n",
        "except Exception as e:\n",
        "    print(\"Aviso: não foi possível escrever abas de TAX (ok seguir).\", e)\n",
        "\n",
        "PATCH C — Filtro por mode\n",
        "\n",
        "No bloco onde você prepara df a partir da planilha, logo após:\n",
        "\n",
        "df = df.dropna(subset=[\"selection\"])\n",
        "df = df[df[\"stage\"].isin([\"preselection\",\"selection_final\"])]\n",
        "\n",
        "\n",
        "adicione:\n",
        "\n",
        "if MODE_FILTER_ENABLED and \"mode\" in df.columns:\n",
        "    df = df[df[\"mode\"].isin(MODE_ALLOWED)]\n",
        "\n",
        "PATCH D — Proporções suavizadas e ENTROPIA\n",
        "\n",
        "Dentro de score_group substitua a criação de linhas do agrupamento por:\n",
        "\n",
        "for k, sub in g:\n",
        "    k2  = \"—\" if pd.isna(k) or str(k).strip()==\"\" else str(k)\n",
        "    pos = (sub[\"valence\"]==\"positive\").sum()\n",
        "    neg = (sub[\"valence\"]==\"negative\").sum()\n",
        "    sc  = sub[\"w\"].sum()\n",
        "    denom = pos + neg + 2*SMOOTH\n",
        "    rows_.append({\n",
        "        \"attr\": col, \"value\": k2, \"score\": sc, \"pos\": pos, \"neg\": neg, \"count\": len(sub),\n",
        "        \"pos_rate\": round((pos + SMOOTH)/denom, 4),\n",
        "        \"neg_rate\": round((neg + SMOOTH)/denom, 4),\n",
        "    })\n",
        "\n",
        "\n",
        "E logo após calcular neg_share, adicione ENTROPIA:\n",
        "\n",
        "from math import log2\n",
        "def _ent(v):\n",
        "    if not v: return None\n",
        "    total = sum(v) + SMOOTH*len(v)\n",
        "    H = 0.0\n",
        "    for c in v:\n",
        "        p = (c + SMOOTH)/total\n",
        "        H += -p*(log2(p) if p>0 else 0.0)\n",
        "    Hmax = log2(len(v)) if len(v)>1 else 1.0\n",
        "    return round(H/Hmax, 4)\n",
        "\n",
        "finp = base[(base[\"stage\"]==\"selection_final\") & (base[\"valence\"]==\"positive\")]\n",
        "ent_dom  = _ent(finp[\"dominio\"].value_counts().tolist())\n",
        "ent_chk  = _ent(finp[\"chakra\"].value_counts().tolist())\n",
        "\n",
        "\n",
        "E inclua no return:\n",
        "\n",
        "return out, {\"retencao_pos\": ret, \"neg_share\": neg_share,\n",
        "             \"entropy_dom_final\": ent_dom, \"entropy_chakra_final\": ent_chk}\n",
        "\n",
        "PATCH E — Aplicar MIN_N e nova aba ENTROPIA\n",
        "\n",
        "Na função aggregate_company, mude a assinatura:\n",
        "\n",
        "def aggregate_company(expanded_rows, weights=None, catalog=None, smooth=1.0, min_n=5, apply_min_filter=False):\n",
        "\n",
        "\n",
        "Crie ent_rows e preencha:\n",
        "\n",
        "ent_rows = []\n",
        "# dentro do loop por (tenant,team):\n",
        "ent_rows.append({\n",
        "    \"tenant_id\": tenant or \"DEFAULT\",\n",
        "    \"team\": team or \"DEFAULT\",\n",
        "    \"entropy_dom_final\": metrics[\"entropy_dom_final\"],\n",
        "    \"entropy_chakra_final\": metrics[\"entropy_chakra_final\"]\n",
        "})\n",
        "\n",
        "\n",
        "Após montar HEATMAP e ATTRS, aplique filtro opcional:\n",
        "\n",
        "if apply_min_filter:\n",
        "    ok = set(map(tuple, VIESES.query(\"n_min_ok==True\")[[\"tenant_id\",\"team\"]].to_numpy()))\n",
        "    if len(HEATMAP):\n",
        "        HEATMAP = HEATMAP[[ (r.tenant_id, r.team) in ok for _, r in HEATMAP.iterrows() ]]\n",
        "    if len(ATTRS):\n",
        "        ATTRS   = ATTRS[[   (r.tenant_id, r.team) in ok for _, r in ATTRS.iterrows() ]]\n",
        "ENTROPIA = pd.DataFrame(ent_rows).sort_values([\"tenant_id\",\"team\"]).reset_index(drop=True)\n",
        "return {\"OVERVIEW\": OVERVIEW, \"HEATMAP\": HEATMAP, \"RETENCAO\": RETENCAO, \"VIESES\": VIESES, \"ATTRS\": ATTRS, \"ENTROPIA\": ENTROPIA}\n",
        "\n",
        "\n",
        "E, na escrita:\n",
        "\n",
        "write_tab(ss_out, \"CULT_ENTROPIA\", packs[\"ENTROPIA\"])\n",
        "\n",
        "PATCH F — “Desejado” focado e escrito\n",
        "\n",
        "Substitua a seleção da base para desejado:\n",
        "\n",
        "base_for_desired = expanded_enriched\n",
        "if DESIRED_FROM_FINAL_POS:\n",
        "    base_for_desired = base_for_desired[(base_for_desired[\"stage\"]==\"selection_final\") & (base_for_desired[\"valence\"]==\"positive\")]\n",
        "DES = build_desired_auto(base_for_desired, TAX, k_total=12)\n",
        "\n",
        "\n",
        "E depois de gerar DES e os GAP_*, grave na Sheets se habilitado:\n",
        "\n",
        "if WRITE_DESIRED_TO_SHEET:\n",
        "    write_tab(ss_out, \"CULT_DESEJADO\", DES[[\"essencia\",\"tema_primario\",\"barrett_principal\",\"camada\"]])\n",
        "    write_tab(ss_out, \"CULT_GAP_BARRETT\", GAP_BARRETT.reset_index())\n",
        "    write_tab(ss_out, \"CULT_GAP_TEMAS\",   GAP_TEMAS.reset_index())\n",
        "    write_tab(ss_out, \"CULT_GAP_CAPS\",    GAP_CAPS.reset_index())\n",
        "\n",
        "PATCH G — MD: entropia e taxas\n",
        "\n",
        "No bloco de export MD, após “Destaques globais…”, acrescente:\n",
        "\n",
        "lines.append(\"## Entropia por time (FINAL +)\\n\")\n",
        "lines.append(md_tbl(packs[\"ENTROPIA\"]))\n",
        "lines.append(\"## HEATMAP com proporções suavizadas\\n\")\n",
        "cols = [\"tenant_id\",\"team\",\"attr\",\"value\",\"score\",\"count\",\"pos\",\"neg\",\"pos_rate\",\"neg_rate\"]\n",
        "hm = packs[\"HEATMAP\"][cols] if set(cols).issubset(packs[\"HEATMAP\"].columns) else packs[\"HEATMAP\"]\n",
        "lines.append(md_tbl(hm))\n",
        "\n",
        "\n",
        "Esses patches mantêm a polaridade ±, integram Barrett e adicionam controles de viés e leitura executiva coerentes com o método."
      ],
      "metadata": {
        "id": "Vl9XAsWpp2F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1) Cultura — adicionar após o bloco “Export MD/PDF”\n",
        "\n",
        "# === AI PROMPT — CULTURA (salva .txt e escreve na planilha) ===\n",
        "from datetime import datetime\n",
        "\n",
        "def _md(df):\n",
        "    import pandas as pd\n",
        "    if df is None or len(df)==0: return \"_sem dados_\\n\"\n",
        "    try:\n",
        "        return df.to_markdown(index=False) + \"\\n\"\n",
        "    except Exception:\n",
        "        return pd.DataFrame(df).to_markdown(index=False)+\"\\n\"\n",
        "\n",
        "def _safe_cols(df, cols):\n",
        "    have = [c for c in cols if c in df.columns]\n",
        "    return df[have] if have else df\n",
        "\n",
        "TENANT_F = \"ALL\"\n",
        "TEAM_F   = \"ALL\"\n",
        "MODE_F   = \"ALL\"\n",
        "PERIODO  = \"não informado\"\n",
        "\n",
        "try:\n",
        "    # Se você tiver filtros definidos no notebook, preencha aqui\n",
        "    TENANT_F = str(TENANT_FILTER) if 'TENANT_FILTER' in globals() else TENANT_F\n",
        "    TEAM_F   = str(TEAM_FILTER)   if 'TEAM_FILTER'   in globals() else TEAM_F\n",
        "    MODE_F   = \",\".join(MODE_ALLOWED) if 'MODE_ALLOWED' in globals() else MODE_F\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Tabelas base (packs gerado anteriormente)\n",
        "VIESES    = packs.get(\"VIESES\")\n",
        "RETENCAO  = packs.get(\"RETENCAO\")\n",
        "OVERVIEW  = packs.get(\"OVERVIEW\")\n",
        "HEATMAP   = packs.get(\"HEATMAP\")\n",
        "ATTRS     = packs.get(\"ATTRS\")\n",
        "ENTROPIA  = packs.get(\"ENTROPIA\")\n",
        "\n",
        "# Desejado/GAP se existirem\n",
        "DES_TAB   = globals().get(\"DES\", None)\n",
        "GAP_BAR   = globals().get(\"GAP_BARRETT\", None)\n",
        "GAP_TEM   = globals().get(\"GAP_TEMAS\", None)\n",
        "GAP_CAP   = globals().get(\"GAP_CAPS\", None)\n",
        "\n",
        "# Colunas “essenciais” para HEATMAP no prompt\n",
        "hm_cols = [\"tenant_id\",\"team\",\"attr\",\"value\",\"score\",\"count\",\"pos\",\"neg\",\"pos_rate\",\"neg_rate\"]\n",
        "HEATMAP_VIEW = _safe_cols(HEATMAP, hm_cols)\n",
        "\n",
        "prompt = []\n",
        "prompt.append(\"# Prompt de Análise de Cultura — v1 (preenchido)\\n\")\n",
        "prompt.append(\"Você é um analista sênior de cultura. Gere diagnóstico e plano de ação a partir das tabelas a seguir.\\n\")\n",
        "prompt.append(f\"Filtros: tenant_id={TENANT_F}, team={TEAM_F}, mode={MODE_F}, período={PERIODO}\\n\\n\")\n",
        "\n",
        "prompt.append(\"## CULT_VIESES\\n\");    prompt.append(_md(VIESES))\n",
        "prompt.append(\"## CULT_RETENCAO\\n\");  prompt.append(_md(RETENCAO))\n",
        "prompt.append(\"## CULT_OVERVIEW\\n\");  prompt.append(_md(OVERVIEW))\n",
        "prompt.append(\"## CULT_HEATMAP\\n\");   prompt.append(_md(HEATMAP_VIEW))\n",
        "prompt.append(\"## CULT_ATTRS\\n\");     prompt.append(_md(ATTRS))\n",
        "if ENTROPIA is not None:\n",
        "    prompt.append(\"## CULT_ENTROPIA\\n\"); prompt.append(_md(ENTROPIA))\n",
        "if DES_TAB is not None:\n",
        "    prompt.append(\"## CULT_DESEJADO\\n\"); prompt.append(_md(DES_TAB[[\"essencia\",\"tema_primario\",\"barrett_principal\",\"camada\"]]))\n",
        "if GAP_BAR is not None:\n",
        "    prompt.append(\"## CULT_GAP_BARRETT\\n\"); prompt.append(_md(GAP_BAR.reset_index()))\n",
        "if GAP_TEM is not None:\n",
        "    prompt.append(\"## CULT_GAP_TEMAS\\n\");   prompt.append(_md(GAP_TEM.reset_index()))\n",
        "if GAP_CAP is not None:\n",
        "    prompt.append(\"## CULT_GAP_CAPS\\n\");    prompt.append(_md(GAP_CAP.reset_index()))\n",
        "\n",
        "prompt.append(\"\\n## Instruções ao modelo\\n\")\n",
        "prompt.append(\"1) Valide n_min_ok e pct_negativos; 2) Classifique retenção (<30, 30–60, ≥60); \")\n",
        "prompt.append(\"3) Liste Top 3 por score em Barrett, Temas, Capacidades, Domínio, Chakra, Camada, indicando tração vs resistência via pos/neg_rate; \")\n",
        "prompt.append(\"4) Leia ENTROPIA (diversidade vs monocultura); 5) Síntese por time; 6) 1–3 alavancas por time com métrica; \")\n",
        "prompt.append(\"7) Conecte ações aos maiores GAPs; 8) Alerte vieses/limitações e proponha reavaliação em 4–6 semanas.\\n\")\n",
        "\n",
        "PROMPT_TXT = \"\\n\".join(prompt)\n",
        "out_path = f\"/mnt/data/prompt_cultura_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt\"\n",
        "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(PROMPT_TXT)\n",
        "print(\"Prompt CULTURA salvo em:\", out_path)\n",
        "\n",
        "# Grava na Sheets (aba CULT_PROMPT)\n",
        "try:\n",
        "    import pandas as pd\n",
        "    write_tab(ss_out, \"CULT_PROMPT\", pd.DataFrame([{\"prompt\": PROMPT_TXT}]))\n",
        "except Exception as e:\n",
        "    print(\"Aviso: não foi possível escrever CULT_PROMPT:\", e)\n",
        "\n",
        "--------------------------------------\n",
        "Hoje sai em .txt e na Sheets. No Colab só aparece a mensagem “salvo em…”.\n",
        "Se quiser ver o texto completo na saída do Colab, adicione estas linhas ao fim de cada bloco.\n",
        "\n",
        "Cultura\n",
        "\n",
        "Logo após print(\"Prompt CULTURA salvo em:\", out_path):\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(\"### Pré-visualização do prompt (CULTURA)\"))\n",
        "display(Markdown(f\"```text\\n{PROMPT_TXT}\\n```\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "1P6VbFrnCWTg",
        "outputId": "f38e3990-7ca8-44a6-e381-a2af365fb72f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unmatched ')' (ipython-input-362446309.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-362446309.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1) Cultura — adicionar após o bloco “Export MD/PDF”\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sXd9QvJzCXHi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}